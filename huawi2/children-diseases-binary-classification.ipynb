{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LazyLoader' from 'keras.utils.generic_utils' (C:\\Users\\Dell\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\utils\\generic_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8244/3517946769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msquared_hinge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\saving\\saved_model\\metric_serialization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayer_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\saving\\saved_model\\base_serialization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_contextlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LazyLoader' from 'keras.utils.generic_utils' (C:\\Users\\Dell\\.conda\\envs\\my-env\\lib\\site-packages\\keras\\utils\\generic_utils.py)"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import os, shutil, stat\n",
    "import glob as gb\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement keras.api\n",
      "ERROR: No matching distribution found for keras.api\n"
     ]
    }
   ],
   "source": [
    "!pip install keras.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dell\\.conda\\envs\\my-env\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\dell\\.conda\\envs\\my-env\\lib\\site-packages (from opencv-python) (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we get the dataset (my first dataset too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Dell\\\\Desktop\\\\huawi\\\\' #to get the path of folders \n",
    "Normalimages = os.listdir(path + \"\\\\normal\")\n",
    "AbnormalImages = os.listdir(path + \"\\\\abnormal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we get to the 2 folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in  os.listdir(path) :    \n",
    "    files = os.listdir(os.path.join(path,folder))\n",
    "    print(f'For data , found {len(files)} in folder {folder}')  #to get the numbers of images in each folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We must make our dictionary for labeling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'normal':0 ,'abnormal':1}   #Dictionary  for labeling data\n",
    "\n",
    "def getcode(n) : \n",
    "   \n",
    "            return code[n]     #get the code of data from dictionary\n",
    "        \n",
    "def getLabel(n):\n",
    "    for x,c in new_labels.items():\n",
    "        if n==c:\n",
    "            return x\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we read the images from 2 folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = []\n",
    "images=[] # to store images\n",
    "labels=[] # to store label for each image\n",
    "for folder in  os.listdir(path) : \n",
    "    files = os.listdir(os.path.join(path,folder))\n",
    "    \n",
    "    \n",
    "    for file in tqdm(files): \n",
    "        image=plt.imread(os.path.join(path,folder+\"/\"+file)) \n",
    "        images.append(image) # add image into list \n",
    "        labels.append(folder) # to add label into list \n",
    "#         size.append(image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(images ), len(labels),labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot to know the number of images in each flder and mak a simple comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show \n",
    "sns.countplot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we show a sample of images.\n",
    "# Note 1 for abnormal and 0 for normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(images),36))) : \n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.imshow(images[i])    \n",
    "    plt.axis('off')\n",
    "    plt.title(getcode(labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make the list of images into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "images=np.array(images)\n",
    "labels=np.array(labels)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(images),36))) : \n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.imshow(images[i])    \n",
    "    plt.axis('off')\n",
    "    plt.title(getcode(labels[i]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The shape of images isn't the same so i resize them to make them having same size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize images into 224 ,224, 3\n",
    "image_size=224\n",
    "\n",
    "new_images=[]\n",
    "new_labels=[]\n",
    "\n",
    "for i in range(len(images)):\n",
    "    \n",
    "    if images[i].shape[-1]!=image_size : # to exclude it image that has 1 channel (grey scale)\n",
    "        if images[i].shape[-1] ==3: # to select the image that has 3 channel  (some of them have 4)\n",
    "            new_images.append(cv2.resize(images[i] , (image_size,image_size)) )\n",
    "            new_labels.append(labels[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(new_images),36))) : \n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.imshow(new_images[i])    \n",
    "    plt.axis('off')\n",
    "    plt.title(getcode(new_labels[i]))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert labels to one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to one hot encoder\n",
    "new_labels=[code[item] for item in new_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this section i make the resized images into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to array , and rescale images \n",
    "new_labels=np.array(new_labels)\n",
    "\n",
    "new_images=np.array(new_images)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(new_images),36))) : \n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.imshow(new_images[i])    \n",
    "    plt.axis('off')\n",
    "    plt.title(new_labels[i])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we finish the data preparation and scaling.\n",
    "# Let's split it into train, validate and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# split entire data to x_train , x_test , y_train , y_test \n",
    "x_train , x_test , y_train , y_test =train_test_split(new_images , new_labels , test_size=0.2 , shuffle=True , stratify=new_labels)\n",
    "\n",
    "# split entire train to x_train , x_val , y_train , y_val\n",
    "\n",
    "x_train , x_val , y_train , y_val=train_test_split(x_train , y_train , test_size=0.05 , shuffle=True , stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for n , i in enumerate(list(np.random.randint(0,len(y_train),36))) : \n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.imshow(x_train[i])    \n",
    "    plt.axis('off')\n",
    "    plt.title(y_train[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "generator=ImageDataGenerator(\n",
    "       \n",
    "      rotation_range=90,\n",
    "      width_shift_range=0.1,\n",
    "      height_shift_range=0.1,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest'\n",
    ")\n",
    "\n",
    "batch_size=50                                            \n",
    "train_generator=generator.flow(x_train,y_train ,batch_size=batch_size)\n",
    "val_generator=generator.flow(x_val,y_val)\n",
    "\n",
    "#get data for generator\n",
    "train_data=train_generator.__getitem__(0)[0]\n",
    "train_labels=train_generator.__getitem__(0)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# plt.figure(figsize=(20,20))\n",
    "# for n , i in enumerate(list(np.random.randint(0,32,36))) : \n",
    "#     plt.subplot(6,6,n+1)\n",
    "#     plt.imshow(train_data[i])    \n",
    "#     plt.axis('off')\n",
    "#     plt.title(train_labels[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "# # YOUR CODE HERE\n",
    "# tf.keras.layers.Conv2D(1024,kernel_size=(3,3),activation='relu',input_shape=(image_size,image_size,3)),\n",
    "# tf.keras.layers.Conv2D(512,kernel_size=(3,3),activation='relu'),\n",
    "# tf.keras.layers.AvgPool2D(4,4),\n",
    "# tf.keras.layers.Conv2D(128,kernel_size=(3,3),activation='relu'),    \n",
    "# tf.keras.layers.Conv2D(80,kernel_size=(3,3),activation='relu'),    \n",
    "# tf.keras.layers.Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
    "# tf.keras.layers.MaxPool2D(4,4),\n",
    "# tf.keras.layers.Flatten(),\n",
    "# #Full Connected Layers\n",
    "# tf.keras.layers.Dense(512, activation='relu'),\n",
    "# #Add dropout to avoid Overfit\n",
    "# tf.keras.layers.Dropout(0.25),\n",
    "# tf.keras.layers.Dense(256, activation='relu'),\n",
    "# tf.keras.layers.Dense(128, activation='relu'),\n",
    "# #Add dropout to avoid Overfit\n",
    "# tf.keras.layers.Dropout(0.4),\n",
    "# tf.keras.layers.Dense(64, activation='relu'),\n",
    "# tf.keras.layers.Dense(1 , activation='sigmoid'),\n",
    "       \n",
    "# ])\n",
    "\n",
    "# print(model.summary())\n",
    "# model.compile(optimizer='adam', loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "# epochs = 64\n",
    "# history = model.fit(x_train , y_train ,batch_size=32,epochs=epochs , validation_data=(x_val , y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install q tensorflow==2.1\n",
    "!pip install q keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from keras.applications import VGG16\n",
    "\n",
    "# pretrain_model = VGG16(input_shape = (image_size, image_size, 3), weights = 'imagenet', include_top = False)\n",
    "\n",
    "# for layer in pretrain_model.layers:\n",
    "#     layer.trainable = False\n",
    "    \n",
    "# pretrain_model.summary()    \n",
    "\n",
    "# from keras.applications import ResNet50\n",
    "\n",
    "# # resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# pretrain_model=ResNet50(include_top=False,input_shape=(image_size,image_size,3),pooling='max',classes=2, weights = 'imagenet')\n",
    "# for layer in pretrain_model.layers:\n",
    "#     layer.trainable = False\n",
    "# pretrain_model.summary()    \n",
    "\n",
    "\n",
    "import efficientnet.keras as efn \n",
    "pretrain_model = efn.EfficientNetB3(\n",
    "    weights='imagenet',\n",
    "    input_shape=(image_size,image_size,3),\n",
    "    include_top=False,\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "pretrain_model.summary()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "x = tf.keras.layers.Flatten()(pretrain_model.output)\n",
    "\n",
    "#Full Connected Layers\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "#Add dropout to avoid Overfit\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "#Add dropout to avoid Overfit\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "x=tf.keras.layers.Dense(1 , activation='sigmoid')(x)\n",
    "       \n",
    "\n",
    "model = Model( pretrain_model.input, x) \n",
    "\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "history = model.fit(x_train , y_train ,batch_size=20,epochs=epochs , validation_data=(x_val , y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saveModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from keras.models import load_model\n",
    "loadedModel=load_model(\"./saveModel.h5\")\n",
    "loadedModel.compile(optimizer='adam', loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred=[]\n",
    "for p in pred:\n",
    "    if (p >= 0.5):\n",
    "        new_pred.append(1)\n",
    "    else:\n",
    "        new_pred.append(0)\n",
    "        \n",
    "print(new_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['normal', 'abnormal']\n",
    "print(classification_report(new_pred,y_test, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(new_pred,y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gellabel(arr):\n",
    "    new_arr=[]\n",
    "    for p in arr:\n",
    "        if (p == 0):\n",
    "            new_arr.append('normal')\n",
    "        else:\n",
    "            new_arr.append('abnormal')\n",
    "        \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test= gellabel(y_test)\n",
    "new_new_pred=gellabel(new_pred)\n",
    "plt.figure(figsize=(30,30))\n",
    "for n , i in enumerate(list(np.random.randint(0,32,36))) : \n",
    "    plt.subplot(6,6,n+1)\n",
    "    plt.imshow(x_test[i])    \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"   Real: {(new_y_test[i])   } Vs  Predict: {(new_new_pred[i])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
